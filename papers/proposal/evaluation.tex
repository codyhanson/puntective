\section{Evaluation}

In order to judge the effectiveness of the system, we will compare the results of the pun classification to that of a human expert. We will build a dataset that contains a mixture of text snippets that contain puns, and some that do not. Also, because not all puns are created equal, we will seek out puns of varying complexity to attempt to highlight limitations of our techniques. To ensure that the evaluation is fair, the texts in the dataset will be selected with as much randomness as possible, from a pool of candidates.

It will be important to track statistics on success rates, including false positives. False positives will indicate that the techniques are not refined enough, or are going down a path of incorrect analysis.

Speed of execution will not me a main goal or metric for this research. It is hopeful that search techniques will take at most on the order of minutes, but if the complexity of the analysis become of too high an order, those techniques will need to be rethought.

